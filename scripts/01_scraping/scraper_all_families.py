"""
Script para scrapear TODAS las familias desde la categor√≠a principal

Este script:
1. Lee la p√°gina de Categor√≠a:Familias_de_Chile
2. Extrae todas las subcategor√≠as de familias
3. Scrapea cada familia usando scraper_categories.py
4. Guarda un archivo consolidado con todas las familias

Uso:
    python scraper_all_families.py
    python scraper_all_families.py --limit 5  # Scrapear solo primeras 5 familias (para testing)
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import argparse
from concurrent.futures import ProcessPoolExecutor, as_completed
from scraper_categories import (
    scrape_family_from_category,
    save_data,
    BASE_URL,
    get_output_path,
    get_project_root,
    get_headers,
    get_api_headers,
)
from urllib.parse import quote

def get_soup(url, retries=3, base_delay=2, rate_limit_delay=60):
    """Obtiene BeautifulSoup de una URL con reintentos y backoff."""
    for i in range(retries):
        try:
            resp = requests.get(url, headers=get_headers(), timeout=15)
            if resp.status_code in (403, 429):
                retry_after = resp.headers.get('Retry-After')
                wait = (
                    int(retry_after)
                    if retry_after and retry_after.isdigit()
                    else rate_limit_delay * (i + 1)
                )
                print(f"‚ö†Ô∏è  Rate limit en {url}. Esperando {wait}s (intento {i+1}/{retries})")
                time.sleep(wait)
                continue
            resp.raise_for_status()
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"‚ùå Error: {e}. Reintento {i+1}/{retries}")
            time.sleep(base_delay * (i + 1))
    return None


COUNTRY_CATEGORY = {
    "chile": "Familias_de_Chile",
    "argentina": "Familias_de_Argentina",
    "mexico": "Familias_de_M√©xico",
    "peru": "Familias_de_Per√∫",
    "colombia": "Familias_de_Colombia",
    "venezuela": "Familias_de_Venezuela",
    "uruguay": "Familias_de_Uruguay",
    "bolivia": "Familias_de_Bolivia",
    "ecuador": "Familias_de_Ecuador",
    "paraguay": "Familias_de_Paraguay",
}


def extract_family_categories(country="chile", custom_category=None):
    """
    Extrae todas las categor√≠as de familias desde una categor√≠a
    
    Args:
        country: Pa√≠s (chile, argentina, mexico)
        custom_category: Categor√≠a personalizada (ej: "Familias_pol√≠ticas_de_Argentina")
    
    Returns:
        list: Lista de diccionarios con {nombre, url}
    """
    api_url = f"{BASE_URL}/w/api.php"
    
    if custom_category:
        category_key = custom_category
    else:
        category_key = COUNTRY_CATEGORY.get(country.lower(), None)
        if not category_key:
            raise ValueError(f"Pa√≠s no soportado: {country}")
    
    category_title = f"Categor√≠a:{category_key}"

    print(f"\nüîç Buscando categor√≠as de familias en: {BASE_URL}/wiki/{category_title}")
    print("=" * 80)

    families = []
    params = {
        'action': 'query',
        'list': 'categorymembers',
        'cmtitle': category_title,
        'cmtype': 'subcat',
        'cmlimit': 'max',
        'format': 'json'
    }

    while True:
        resp = requests.get(api_url, headers=get_api_headers(), params=params, timeout=15)
        if resp.status_code in (403, 429):
            retry_after = resp.headers.get('Retry-After')
            wait = int(retry_after) if retry_after and retry_after.isdigit() else 60
            print(f"‚ö†Ô∏è  Rate limit en API. Esperando {wait}s")
            time.sleep(wait)
            continue
        resp.raise_for_status()
        data = resp.json()

        for item in data.get('query', {}).get('categorymembers', []):
            title = item.get('title', '').strip()
            if not title:
                continue
            if 'familia' not in title.lower():
                continue
            url_title = quote(title.replace(' ', '_'))
            full_url = f"{BASE_URL}/wiki/{url_title}"
            families.append({
                'nombre': title.replace('Categor√≠a:', ''),
                'url': full_url
            })
            print(f"  ‚úì {title.replace('Categor√≠a:', '')}")

        if 'continue' in data:
            params.update(data['continue'])
        else:
            break
    
    print(f"\n‚úÖ Total de familias encontradas: {len(families)}")
    return families


def scrape_and_save_family(family, resume=False, country="chile"):
    """Scrapea una familia y guarda el CSV individual."""
    try:
        df, family_name = scrape_family_from_category(
            family['url'],
            family['nombre'],
            resume=resume,
            country=country
        )
        if df is not None and not df.empty:
            save_data(df, family_name, country=country)
            return {"status": "success", "family": family['nombre']}
        return {"status": "empty", "family": family['nombre']}
    except Exception as e:
        return {"status": "error", "family": family['nombre'], "error": str(e)}


def scrape_all_families(limit=None, workers=1, resume=False, country="chile", custom_category=None):
    """
    Scrapea todas las familias
    
    Args:
        limit: N√∫mero m√°ximo de familias a scrapear (None = todas)
        country: Pa√≠s (chile, argentina, mexico)
        custom_category: Categor√≠a personalizada (ej: "Familias_pol√≠ticas_de_Argentina")
    """
    print("\n" + "=" * 80)
    category_name = custom_category if custom_category else COUNTRY_CATEGORY.get(country.lower(), country)
    print(f"üöÄ SCRAPING MASIVO DE FAMILIAS ({country.upper()})")
    if custom_category:
        print(f"üìÇ Categor√≠a: {custom_category}")
    print("=" * 80)
    
    # Obtener lista de familias
    families = extract_family_categories(country=country, custom_category=custom_category)
    
    if not families:
        print("‚ùå No se encontraron familias para scrapear")
        return
    
    # Aplicar l√≠mite si se especific√≥
    if limit:
        families = families[:limit]
        print(f"\n‚ö†Ô∏è  Limitando a las primeras {limit} familias")

    families_all = list(families)
    if resume:
        remaining = []
        skipped = 0
        for family in families:
            output_path = get_output_path(family['nombre'])
            if output_path.exists():
                skipped += 1
            else:
                remaining.append(family)
        families = remaining
        print(f"\n‚Ü©Ô∏è  Reanudar activo: {skipped} familias ya exist√≠an")

    # Scrapear cada familia
    successful = 0
    failed = 0

    if workers and workers > 1:
        print(f"\n‚öôÔ∏è  Modo paralelo activado: {workers} workers")
        with ProcessPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(scrape_and_save_family, family, resume, country): family
                for family in families
            }
            for future in as_completed(futures):
                result = future.result()
                if result["status"] == "success":
                    successful += 1
                    print(f"‚úÖ {result['family']}: OK")
                elif result["status"] == "empty":
                    failed += 1
                    print(f"‚ö†Ô∏è  {result['family']}: Sin datos")
                else:
                    failed += 1
                    print(f"‚ùå Error en {result['family']}: {result.get('error')}")
    else:
        for i, family in enumerate(families, 1):
            print(f"\n{'=' * 80}")
            print(f"[{i}/{len(families)}] Procesando: {family['nombre']}")
            print(f"{'=' * 80}")

            result = scrape_and_save_family(family, resume=resume, country=country)
            if result["status"] == "success":
                successful += 1
                print(f"‚úÖ {family['nombre']}: OK")
            elif result["status"] == "empty":
                failed += 1
                print(f"‚ö†Ô∏è  {family['nombre']}: Sin datos")
            else:
                failed += 1
                print(f"‚ùå Error en {family['nombre']}: {result.get('error')}")

            # Delay entre familias
            time.sleep(2)

    # Consolidar todos los datos
    output_dir = get_project_root() / "data" / "raw" / country.lower() / "familias"
    output_files = []
    for family in families_all:
        output_path = get_output_path(family['nombre'], country=country)
        if output_path.exists():
            output_files.append(output_path)

    if output_files:
        print("\n" + "=" * 80)
        print("üìä Consolidando datos...")

        all_data = []
        for path in output_files:
            try:
                df = pd.read_csv(path, sep=';', encoding='utf-8')
                if not df.empty:
                    all_data.append(df)
            except Exception as e:
                print(f"‚ö†Ô∏è  No se pudo leer {path}: {e}")

        if all_data:
            df_consolidated = pd.concat(all_data, ignore_index=True)

            # Eliminar duplicados (personas que aparecen en m√∫ltiples familias)
            if 'url' in df_consolidated.columns:
                df_consolidated.drop_duplicates(subset=['url'], keep='first', inplace=True)

            # Guardar archivo consolidado
            consolidated_file = output_dir / "_CONSOLIDADO_todas_familias.csv"
            df_consolidated.to_csv(consolidated_file, index=False, sep=';', encoding='utf-8')

            print(f"\n" + "=" * 80)
            print("‚úÖ SCRAPING COMPLETADO")
            print("=" * 80)
            print(f"   Familias exitosas: {successful}")
            print(f"   Familias fallidas: {failed}")
            print(f"   Total de personas: {len(df_consolidated)}")
            print(f"   Archivo consolidado: {consolidated_file}")
            print("=" * 80)
        else:
            print("\n‚ùå No se pudo extraer ning√∫n dato")
    else:
        print("\n‚ùå No hay archivos disponibles para consolidar")


def main():
    parser = argparse.ArgumentParser(
        description='Scrapear todas las familias chilenas desde Wikipedia'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='N√∫mero m√°ximo de familias a scrapear (√∫til para testing)'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=1,
        help='N√∫mero de procesos en paralelo (default: 1)'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='Reanudar saltando familias ya descargadas'
    )
    parser.add_argument(
        '--country',
        type=str,
        default='chile',
        help='Pa√≠s a scrapear (chile, argentina, mexico)'
    )
    parser.add_argument(
        '--category-custom',
        type=str,
        default=None,
        help='Categor√≠a personalizada (ej: "Familias_pol√≠ticas_de_Argentina")'
    )
    
    args = parser.parse_args()
    
    scrape_all_families(
        limit=args.limit, 
        workers=args.workers, 
        resume=args.resume, 
        country=args.country,
        custom_category=args.category_custom
    )


if __name__ == "__main__":
    main()
