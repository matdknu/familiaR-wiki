"""
Script para scrapear TODAS las familias desde la categor√≠a principal

Este script:
1. Lee la p√°gina de Categor√≠a:Familias_de_Chile
2. Extrae todas las subcategor√≠as de familias
3. Scrapea cada familia usando scraper_categories.py
4. Guarda un archivo consolidado con todas las familias

Uso:
    python scraper_all_families.py
    python scraper_all_families.py --limit 5  # Scrapear solo primeras 5 familias (para testing)
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import argparse
import os
from scraper_categories import scrape_family_from_category, HEADERS, BASE_URL

def get_soup(url, retries=3):
    """Obtiene BeautifulSoup de una URL con reintentos"""
    for i in range(retries):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            resp.raise_for_status()
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"‚ùå Error: {e}. Reintento {i+1}/{retries}")
            time.sleep(2)
    return None


def extract_family_categories():
    """
    Extrae todas las categor√≠as de familias desde Categor√≠a:Familias_de_Chile
    
    Returns:
        list: Lista de diccionarios con {nombre, url}
    """
    url = f"{BASE_URL}/wiki/Categor√≠a:Familias_de_Chile"
    
    print(f"\nüîç Buscando categor√≠as de familias en: {url}")
    print("=" * 80)
    
    soup = get_soup(url)
    if not soup:
        return []
    
    families = []
    
    # Buscar la secci√≥n de subcategor√≠as
    subcats_section = soup.find('div', {'id': 'mw-subcategories'})
    
    if not subcats_section:
        print("‚ö†Ô∏è  No se encontr√≥ la secci√≥n de subcategor√≠as")
        return []
    
    # Extraer todos los enlaces a categor√≠as de familias
    links = subcats_section.find_all('a')
    
    for link in links:
        href = link.get('href', '')
        text = link.get_text(strip=True)
        
        # Filtrar solo categor√≠as de familias espec√≠ficas
        if href.startswith('/wiki/Categor√≠a:Familia_') or href.startswith('/wiki/Categor√≠a:Famila_'):
            full_url = BASE_URL + href
            families.append({
                'nombre': text.replace('Categor√≠a:', ''),
                'url': full_url
            })
            print(f"  ‚úì {text}")
    
    print(f"\n‚úÖ Total de familias encontradas: {len(families)}")
    return families


def scrape_all_families(limit=None):
    """
    Scrapea todas las familias chilenas
    
    Args:
        limit: N√∫mero m√°ximo de familias a scrapear (None = todas)
    """
    print("\n" + "=" * 80)
    print("üöÄ SCRAPING MASIVO DE FAMILIAS CHILENAS")
    print("=" * 80)
    
    # Obtener lista de familias
    families = extract_family_categories()
    
    if not families:
        print("‚ùå No se encontraron familias para scrapear")
        return
    
    # Aplicar l√≠mite si se especific√≥
    if limit:
        families = families[:limit]
        print(f"\n‚ö†Ô∏è  Limitando a las primeras {limit} familias")
    
    # Crear directorio de salida
    output_dir = "data/raw/chile/familias"
    os.makedirs(output_dir, exist_ok=True)
    
    # Scrapear cada familia
    all_data = []
    successful = 0
    failed = 0
    
    for i, family in enumerate(families, 1):
        print(f"\n{'=' * 80}")
        print(f"[{i}/{len(families)}] Procesando: {family['nombre']}")
        print(f"{'=' * 80}")
        
        try:
            df, family_name = scrape_family_from_category(family['url'], family['nombre'])
            
            if df is not None and not df.empty:
                # Guardar archivo individual por familia
                safe_name = family_name.replace(' ', '_').replace(':', '').lower()
                filename = f"{output_dir}/{safe_name}_completo.csv"
                df.to_csv(filename, index=False, sep=';', encoding='utf-8')
                
                # Agregar al dataset consolidado
                all_data.append(df)
                successful += 1
                
                print(f"‚úÖ {family['nombre']}: {len(df)} personas extra√≠das")
            else:
                failed += 1
                print(f"‚ö†Ô∏è  {family['nombre']}: Sin datos")
        
        except Exception as e:
            failed += 1
            print(f"‚ùå Error en {family['nombre']}: {e}")
        
        # Delay entre familias
        time.sleep(2)
    
    # Consolidar todos los datos
    if all_data:
        print("\n" + "=" * 80)
        print("üìä Consolidando datos...")
        
        df_consolidated = pd.concat(all_data, ignore_index=True)
        
        # Eliminar duplicados (personas que aparecen en m√∫ltiples familias)
        df_consolidated.drop_duplicates(subset=['url'], keep='first', inplace=True)
        
        # Guardar archivo consolidado
        consolidated_file = f"{output_dir}/_CONSOLIDADO_todas_familias.csv"
        df_consolidated.to_csv(consolidated_file, index=False, sep=';', encoding='utf-8')
        
        print(f"\n" + "=" * 80)
        print("‚úÖ SCRAPING COMPLETADO")
        print("=" * 80)
        print(f"   Familias exitosas: {successful}")
        print(f"   Familias fallidas: {failed}")
        print(f"   Total de personas: {len(df_consolidated)}")
        print(f"   Archivo consolidado: {consolidated_file}")
        print("=" * 80)
    else:
        print("\n‚ùå No se pudo extraer ning√∫n dato")


def main():
    parser = argparse.ArgumentParser(
        description='Scrapear todas las familias chilenas desde Wikipedia'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='N√∫mero m√°ximo de familias a scrapear (√∫til para testing)'
    )
    
    args = parser.parse_args()
    
    scrape_all_families(limit=args.limit)


if __name__ == "__main__":
    main()
